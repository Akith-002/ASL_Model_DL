{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\nprint(\"Done!\")\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-09-30T07:37:05.628594Z","iopub.execute_input":"2025-09-30T07:37:05.628816Z","iopub.status.idle":"2025-09-30T07:39:59.184648Z","shell.execute_reply.started":"2025-09-30T07:37:05.628798Z","shell.execute_reply":"2025-09-30T07:39:59.183875Z"}},"outputs":[{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Setup and Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNetV3Large\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom pathlib import Path\nimport shutil\nfrom datetime import datetime\n\nprint(\"🚀 ASL Training on Kaggle\")\nprint(\"=\" * 50)\nprint(f\"TensorFlow Version: {tf.__version__}\")\n\n# Check GPU\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    print(f\"✅ GPU Available: {len(gpus)} GPU(s)\")\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n    print(\"✅ Mixed precision enabled\")\n    HAS_GPU = True\nelse:\n    print(\"❌ No GPU detected\")\n    HAS_GPU = False\n\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:59.185967Z","iopub.execute_input":"2025-09-30T07:39:59.186375Z","iopub.status.idle":"2025-09-30T07:40:12.651781Z","shell.execute_reply.started":"2025-09-30T07:39:59.186354Z","shell.execute_reply":"2025-09-30T07:40:12.650992Z"}},"outputs":[{"name":"stderr","text":"2025-09-30 07:40:01.499086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759218001.667963      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759218001.717593      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🚀 ASL Training on Kaggle\n==================================================\nTensorFlow Version: 2.18.0\n✅ GPU Available: 1 GPU(s)\n✅ Mixed precision enabled\n==================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Find Dataset","metadata":{}},{"cell_type":"code","source":"# Find dataset path\ninput_dir = Path('/kaggle/input/asl-alphabet')\ndataset_path = None\n\nif input_dir.exists():\n    print(\"🔍 Searching for dataset...\")\n    for item in input_dir.iterdir():\n        print(f\"   Found: {item}\")\n        \n        # Look for asl_alphabet_train folder (might be nested)\n        possible_paths = [\n            item / 'asl_alphabet_train',  # Direct path\n            item,  # Root might be the dataset itself\n        ]\n        \n        for path in possible_paths:\n            if path.exists() and path.is_dir():\n                # Check if this directory contains letter folders (A, B, C, etc.)\n                subdirs = [d for d in path.iterdir() if d.is_dir()]\n                letter_dirs = [d for d in subdirs if d.name in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' or d.name in ['space', 'del', 'nothing']]\n                \n                if len(letter_dirs) > 10:  # Should have at least 10 letter folders\n                    dataset_path = str(path)\n                    print(f\"✅ Dataset found with {len(letter_dirs)} letter folders: {dataset_path}\")\n                    break\n        \n        if dataset_path:\n            break\n\nif not dataset_path:\n    print(\"❌ Dataset not found!\")\n    print(\"Expected structure: dataset/A/, dataset/B/, dataset/C/, etc.\")\nelse:\n    print(f\"📁 Using dataset: {dataset_path}\")\n    \n    # Debug: Show what's actually in the dataset\n    dataset_check = Path(dataset_path)\n    print(f\"\\n🔍 Dataset contents:\")\n    for item in sorted(dataset_check.iterdir()):\n        if item.is_dir():\n            count = len(list(item.glob('*.jpg')) + list(item.glob('*.png')))\n            print(f\"  📁 {item.name}: {count} images\")\n        else:\n            print(f\"  📄 {item.name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:40:12.652888Z","iopub.execute_input":"2025-09-30T07:40:12.653479Z","iopub.status.idle":"2025-09-30T07:40:13.085962Z","shell.execute_reply.started":"2025-09-30T07:40:12.653440Z","shell.execute_reply":"2025-09-30T07:40:13.085343Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"🔍 Searching for dataset...\n   Found: /kaggle/input/asl-alphabet/asl_alphabet_test\n   Found: /kaggle/input/asl-alphabet/asl_alphabet_train\n✅ Dataset found with 29 letter folders: /kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\n📁 Using dataset: /kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\n\n🔍 Dataset contents:\n  📁 A: 3000 images\n  📁 B: 3000 images\n  📁 C: 3000 images\n  📁 D: 3000 images\n  📁 E: 3000 images\n  📁 F: 3000 images\n  📁 G: 3000 images\n  📁 H: 3000 images\n  📁 I: 3000 images\n  📁 J: 3000 images\n  📁 K: 3000 images\n  📁 L: 3000 images\n  📁 M: 3000 images\n  📁 N: 3000 images\n  📁 O: 3000 images\n  📁 P: 3000 images\n  📁 Q: 3000 images\n  📁 R: 3000 images\n  📁 S: 3000 images\n  📁 T: 3000 images\n  📁 U: 3000 images\n  📁 V: 3000 images\n  📁 W: 3000 images\n  📁 X: 3000 images\n  📁 Y: 3000 images\n  📁 Z: 3000 images\n  📁 del: 3000 images\n  📁 nothing: 3000 images\n  📁 space: 3000 images\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Prepare dataset","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(input_dir, output_dir):\n    print(f\"📁 Preparing dataset from: {input_dir}\")\n    \n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    \n    # Get all class directories - filter for actual letter/word folders\n    all_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n    \n    # Filter for valid ASL classes (letters + special classes)\n    valid_classes = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ') | {'space', 'del', 'nothing'}\n    class_dirs = [d for d in all_dirs if d.name in valid_classes or len(d.name) == 1]\n    \n    print(f\"Found {len(all_dirs)} total directories, {len(class_dirs)} valid classes\")\n    print(f\"Valid classes: {sorted([d.name for d in class_dirs])}\")\n    \n    if len(class_dirs) == 0:\n        print(\"❌ No valid ASL class folders found!\")\n        print(\"Expected folders: A, B, C, ..., Z, space, del, nothing\")\n        print(\"Available folders:\", [d.name for d in all_dirs])\n        return None\n    \n    splits = ['train', 'val', 'test']\n    split_ratios = [0.7, 0.15, 0.15]\n    \n    for split in splits:\n        (output_path / split).mkdir(exist_ok=True)\n    \n    total_files = 0\n    split_counts = {'train': 0, 'val': 0, 'test': 0}\n    \n    for class_dir in class_dirs:\n        print(f\"Processing: {class_dir.name}\")\n        \n        image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png')) + list(class_dir.glob('*.jpeg'))\n        image_files.sort()\n        \n        if len(image_files) == 0:\n            print(f\"  ⚠️  No images found in {class_dir.name}\")\n            continue\n        \n        print(f\"  Found {len(image_files)} images\")\n        total_files += len(image_files)\n        \n        n_files = len(image_files)\n        train_end = int(n_files * split_ratios[0])\n        val_end = train_end + int(n_files * split_ratios[1])\n        \n        for split in splits:\n            (output_path / split / class_dir.name).mkdir(exist_ok=True)\n        \n        for i, img_file in enumerate(image_files):\n            if i < train_end:\n                split = 'train'\n            elif i < val_end:\n                split = 'val'\n            else:\n                split = 'test'\n            \n            dst = output_path / split / class_dir.name / img_file.name\n            shutil.copy2(img_file, dst)\n            split_counts[split] += 1\n    \n    print(f\"✅ Dataset prepared: {total_files:,} total files\")\n    print(f\"   Train: {split_counts['train']:,}\")\n    print(f\"   Val: {split_counts['val']:,}\")\n    print(f\"   Test: {split_counts['test']:,}\")\n    return str(output_path)\n\n# Prepare the dataset - IMPORTANT: Output must be in /kaggle/working/ (writable)\nprepared_dataset = prepare_dataset(dataset_path, '/kaggle/working/dataset')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:40:13.087471Z","iopub.execute_input":"2025-09-30T07:40:13.087683Z","iopub.status.idle":"2025-09-30T07:48:15.258047Z","shell.execute_reply.started":"2025-09-30T07:40:13.087667Z","shell.execute_reply":"2025-09-30T07:48:15.257307Z"}},"outputs":[{"name":"stdout","text":"📁 Preparing dataset from: /kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\nFound 29 total directories, 29 valid classes\nValid classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\nProcessing: N\n  Found 3000 images\nProcessing: R\n  Found 3000 images\nProcessing: space\n  Found 3000 images\nProcessing: B\n  Found 3000 images\nProcessing: I\n  Found 3000 images\nProcessing: del\n  Found 3000 images\nProcessing: F\n  Found 3000 images\nProcessing: H\n  Found 3000 images\nProcessing: E\n  Found 3000 images\nProcessing: U\n  Found 3000 images\nProcessing: M\n  Found 3000 images\nProcessing: X\n  Found 3000 images\nProcessing: K\n  Found 3000 images\nProcessing: Q\n  Found 3000 images\nProcessing: Y\n  Found 3000 images\nProcessing: S\n  Found 3000 images\nProcessing: G\n  Found 3000 images\nProcessing: A\n  Found 3000 images\nProcessing: O\n  Found 3000 images\nProcessing: T\n  Found 3000 images\nProcessing: V\n  Found 3000 images\nProcessing: Z\n  Found 3000 images\nProcessing: C\n  Found 3000 images\nProcessing: P\n  Found 3000 images\nProcessing: L\n  Found 3000 images\nProcessing: W\n  Found 3000 images\nProcessing: D\n  Found 3000 images\nProcessing: nothing\n  Found 3000 images\nProcessing: J\n  Found 3000 images\n✅ Dataset prepared: 87,000 total files\n   Train: 60,900\n   Val: 13,050\n   Test: 13,050\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Create Dataset Loaders","metadata":{}},{"cell_type":"code","source":"def create_dataset(data_dir, batch_size=32, img_size=(200, 200)):\n    dataset = tf.keras.utils.image_dataset_from_directory(\n        data_dir,\n        seed=42,\n        image_size=img_size,\n        batch_size=batch_size,\n        label_mode='categorical'\n    )\n    return dataset, dataset.class_names\n\ndef augment_dataset(dataset, is_training=True):\n    data_augmentation = tf.keras.Sequential([\n        layers.Rescaling(1./255),\n    ])\n    \n    if is_training:\n        data_augmentation.add(layers.RandomRotation(0.1))\n        data_augmentation.add(layers.RandomZoom(0.1))\n        data_augmentation.add(layers.RandomContrast(0.2))\n        data_augmentation.add(layers.RandomBrightness(0.2))\n    \n    dataset = dataset.map(\n        lambda x, y: (data_augmentation(x), y),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    return dataset.prefetch(tf.data.AUTOTUNE)\n\n# Set parameters\nBATCH_SIZE = 64 if HAS_GPU else 32\nIMG_SIZE = (200, 200)\nOUTPUT_DIR = '/kaggle/working/models'\n\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Image size: {IMG_SIZE}\")\n\n# Load datasets\ntrain_ds, class_names = create_dataset(f\"{prepared_dataset}/train\", BATCH_SIZE, IMG_SIZE)\nval_ds, _ = create_dataset(f\"{prepared_dataset}/val\", BATCH_SIZE, IMG_SIZE)\ntest_ds, _ = create_dataset(f\"{prepared_dataset}/test\", BATCH_SIZE, IMG_SIZE)\n\nprint(f\"✅ Loaded {len(class_names)} classes: {class_names}\")\n\n# Apply augmentation\ntrain_ds = augment_dataset(train_ds, True)\nval_ds = augment_dataset(val_ds, False)\ntest_ds = augment_dataset(test_ds, False)\n\nprint(\"✅ Data augmentation applied\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:48:15.258765Z","iopub.execute_input":"2025-09-30T07:48:15.259025Z","iopub.status.idle":"2025-09-30T07:48:20.569133Z","shell.execute_reply.started":"2025-09-30T07:48:15.259001Z","shell.execute_reply":"2025-09-30T07:48:20.568495Z"}},"outputs":[{"name":"stdout","text":"Batch size: 64\nImage size: (200, 200)\nFound 60900 files belonging to 29 classes.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1759218497.666809      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Found 13050 files belonging to 29 classes.\nFound 13050 files belonging to 29 classes.\n✅ Loaded 29 classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n✅ Data augmentation applied\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"def create_model(num_classes, img_size=(200, 200)):\n    base_model = MobileNetV3Large(\n        input_shape=(*img_size, 3),\n        alpha=1.0,\n        minimalistic=False,\n        include_top=False,\n        weights='imagenet',\n        pooling='avg'\n    )\n    \n    base_model.trainable = False\n    \n    inputs = keras.Input(shape=(*img_size, 3))\n    x = base_model(inputs, training=False)\n    x = layers.Dropout(0.2)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss=keras.losses.CategoricalCrossentropy(),\n        metrics=['accuracy']\n    )\n    \n    return model, base_model\n\n# Calculate class weights\ndef calculate_class_weights(dataset, class_names):\n    class_counts = np.zeros(len(class_names))\n    total_samples = 0\n    \n    for images, labels in dataset:\n        label_indices = tf.argmax(labels, axis=1)\n        for idx in label_indices:\n            class_counts[idx.numpy()] += 1\n        total_samples += len(labels)\n    \n    class_weights = {}\n    for i, count in enumerate(class_counts):\n        if count > 0:\n            class_weights[i] = total_samples / (len(class_names) * count)\n        else:\n            class_weights[i] = 1.0\n    \n    return class_weights\n\n# Create model and calculate weights\nmodel, base_model = create_model(len(class_names), IMG_SIZE)\nclass_weights = calculate_class_weights(train_ds, class_names)\n\nprint(f\"✅ Model created with {model.count_params():,} parameters\")\nprint(f\"✅ Class weights calculated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:48:20.569851Z","iopub.execute_input":"2025-09-30T07:48:20.570054Z","iopub.status.idle":"2025-09-30T07:53:17.819705Z","shell.execute_reply.started":"2025-09-30T07:48:20.570037Z","shell.execute_reply":"2025-09-30T07:53:17.818876Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n\u001b[1m12683000/12683000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n✅ Model created with 3,024,221 parameters\n✅ Class weights calculated\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Train Model - Phase 1","metadata":{}},{"cell_type":"code","source":"# Setup output directory\noutput_path = Path(OUTPUT_DIR)\noutput_path.mkdir(parents=True, exist_ok=True)\n\nprint(\"🎯 Phase 1: Training classifier head\")\n\n# Phase 1 callbacks\ncallbacks_phase1 = [\n    ModelCheckpoint(\n        str(output_path / 'best_model_phase1.keras'),\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    ),\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=5,\n        restore_best_weights=True,\n        verbose=1\n    )\n]\n\n# Train phase 1 (15 epochs)\nhistory_phase1 = model.fit(\n    train_ds,\n    epochs=15,\n    validation_data=val_ds,\n    class_weight=class_weights,\n    callbacks=callbacks_phase1,\n    verbose=1\n)\n\nprint(\"✅ Phase 1 completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:53:17.820407Z","iopub.execute_input":"2025-09-30T07:53:17.820703Z","iopub.status.idle":"2025-09-30T08:33:27.100842Z","shell.execute_reply.started":"2025-09-30T07:53:17.820676Z","shell.execute_reply":"2025-09-30T08:33:27.100226Z"}},"outputs":[{"name":"stdout","text":"🎯 Phase 1: Training classifier head\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1759218808.436136     103 service.cc:148] XLA service 0x7c53c4002660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1759218808.436913     103 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1759218810.880498     103 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  5/952\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 29ms/step - accuracy: 0.0297 - loss: 3.7064 ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1759218817.235495     103 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.0347 - loss: 3.4175\nEpoch 1: val_accuracy improved from -inf to 0.03433, saving model to /kaggle/working/models/best_model_phase1.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 327ms/step - accuracy: 0.0347 - loss: 3.4175 - val_accuracy: 0.0343 - val_loss: 3.3587\nEpoch 2/15\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.0391 - loss: 3.3788\nEpoch 2: val_accuracy improved from 0.03433 to 0.04851, saving model to /kaggle/working/models/best_model_phase1.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 311ms/step - accuracy: 0.0391 - loss: 3.3787 - val_accuracy: 0.0485 - val_loss: 3.3411\nEpoch 3/15\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.0454 - loss: 3.3700\nEpoch 3: val_accuracy improved from 0.04851 to 0.09739, saving model to /kaggle/working/models/best_model_phase1.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 313ms/step - accuracy: 0.0454 - loss: 3.3700 - val_accuracy: 0.0974 - val_loss: 3.3185\nEpoch 4/15\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.0455 - loss: 3.3657\nEpoch 4: val_accuracy did not improve from 0.09739\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 310ms/step - accuracy: 0.0455 - loss: 3.3657 - val_accuracy: 0.0579 - val_loss: 3.3135\nEpoch 5/15\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.0477 - loss: 3.3594\nEpoch 5: val_accuracy did not improve from 0.09739\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 310ms/step - accuracy: 0.0477 - loss: 3.3594 - val_accuracy: 0.0607 - val_loss: 3.3000\nEpoch 6/15\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.0492 - loss: 3.3573\nEpoch 6: val_accuracy did not improve from 0.09739\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 309ms/step - accuracy: 0.0492 - loss: 3.3573 - val_accuracy: 0.0444 - val_loss: 3.3019\nEpoch 7/15\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.0510 - loss: 3.3541\nEpoch 7: val_accuracy did not improve from 0.09739\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 315ms/step - accuracy: 0.0510 - loss: 3.3541 - val_accuracy: 0.0768 - val_loss: 3.2833\nEpoch 8/15\n\u001b[1m950/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.0508 - loss: 3.3514\nEpoch 8: val_accuracy did not improve from 0.09739\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 309ms/step - accuracy: 0.0508 - loss: 3.3514 - val_accuracy: 0.0952 - val_loss: 3.2739\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 3.\n✅ Phase 1 completed\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Train Model - Phase 2","metadata":{}},{"cell_type":"code","source":"print(\"🔥 Phase 2: Fine-tuning entire model\")\n\n# Unfreeze base model\nbase_model.trainable = True\n\n# Recompile with lower learning rate\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.00002),\n    loss=keras.losses.CategoricalCrossentropy(),\n    metrics=['accuracy']\n)\n\n# Phase 2 callbacks\ncallbacks_phase2 = [\n    ModelCheckpoint(\n        str(output_path / 'best_model_final.keras'),\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    ),\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=8,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=0.000001,\n        verbose=1\n    )\n]\n\n# Train phase 2 (15 more epochs) - FIXED with proper epoch counting\ntry:\n    print(\"Starting Phase 2 training...\")\n    history_phase2 = model.fit(\n        train_ds,\n        epochs=30,  # Total epochs (15 + 15)\n        validation_data=val_ds,\n        class_weight=class_weights,\n        callbacks=callbacks_phase2,\n        verbose=1,\n        initial_epoch=15  # Start from epoch 15, train to epoch 30\n    )\n    print(\"✅ Phase 2 completed successfully\")\n    \n    # Check if training actually happened\n    if len(history_phase2.history) == 0 or len(history_phase2.history.get('loss', [])) == 0:\n        print(\"⚠️  Phase 2 didn't train any epochs (check epoch settings)\")\n        raise ValueError(\"No epochs trained in Phase 2\")\n        \nexcept Exception as e:\n    print(f\"⚠️  Phase 2 training failed: {e}\")\n    print(\"Creating empty history for phase 2...\")\n    # Create empty history object\n    class EmptyHistory:\n        def __init__(self):\n            self.history = {}\n    history_phase2 = EmptyHistory()\n\n# Combine histories - Fix for empty Phase 2 history\ncombined_history = {}\n\n# Debug: Print available keys\nprint(\"Phase 1 history keys:\", list(history_phase1.history.keys()))\nprint(\"Phase 2 history keys:\", list(history_phase2.history.keys()))\n\n# Check if Phase 2 training actually happened\nif len(history_phase2.history) == 0:\n    print(\"⚠️  Phase 2 training failed - using only Phase 1 history\")\n    combined_history = history_phase1.history.copy()\nelse:\n    # Normal case - combine both phases\n    common_keys = set(history_phase1.history.keys()) & set(history_phase2.history.keys())\n    print(f\"Common keys: {common_keys}\")\n\n    for key in common_keys:\n        combined_history[key] = history_phase1.history[key] + history_phase2.history[key]\n\n    # Add any missing keys from phase 1\n    for key in history_phase1.history.keys():\n        if key not in combined_history:\n            if 'loss' in history_phase2.history:\n                combined_history[key] = history_phase1.history[key] + [None] * len(history_phase2.history['loss'])\n            else:\n                combined_history[key] = history_phase1.history[key]\n            print(f\"⚠️  Added {key} from phase 1 only\")\n\nprint(\"✅ Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:33:27.101982Z","iopub.execute_input":"2025-09-30T08:33:27.102628Z","iopub.status.idle":"2025-09-30T09:51:34.923260Z","shell.execute_reply.started":"2025-09-30T08:33:27.102605Z","shell.execute_reply":"2025-09-30T09:51:34.922508Z"}},"outputs":[{"name":"stdout","text":"🔥 Phase 2: Fine-tuning entire model\nStarting Phase 2 training...\nEpoch 16/30\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.0465 - loss: 5.2038\nEpoch 16: val_accuracy improved from -inf to 0.03448, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 356ms/step - accuracy: 0.0465 - loss: 5.2023 - val_accuracy: 0.0345 - val_loss: 3.6559 - learning_rate: 2.0000e-05\nEpoch 17/30\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.1613 - loss: 2.9233\nEpoch 17: val_accuracy improved from 0.03448 to 0.09655, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 318ms/step - accuracy: 0.1613 - loss: 2.9231 - val_accuracy: 0.0966 - val_loss: 3.5634 - learning_rate: 2.0000e-05\nEpoch 18/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.3664 - loss: 2.1718\nEpoch 18: val_accuracy improved from 0.09655 to 0.38736, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 315ms/step - accuracy: 0.3664 - loss: 2.1716 - val_accuracy: 0.3874 - val_loss: 2.6067 - learning_rate: 2.0000e-05\nEpoch 19/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.4484 - loss: 1.9009\nEpoch 19: val_accuracy improved from 0.38736 to 0.68008, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 313ms/step - accuracy: 0.4484 - loss: 1.9009 - val_accuracy: 0.6801 - val_loss: 1.3097 - learning_rate: 2.0000e-05\nEpoch 20/30\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.4785 - loss: 1.8121\nEpoch 20: val_accuracy improved from 0.68008 to 0.79188, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 317ms/step - accuracy: 0.4785 - loss: 1.8121 - val_accuracy: 0.7919 - val_loss: 0.9392 - learning_rate: 2.0000e-05\nEpoch 21/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.4939 - loss: 1.7592\nEpoch 21: val_accuracy improved from 0.79188 to 0.85770, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 324ms/step - accuracy: 0.4939 - loss: 1.7592 - val_accuracy: 0.8577 - val_loss: 0.7764 - learning_rate: 2.0000e-05\nEpoch 22/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.4995 - loss: 1.7479\nEpoch 22: val_accuracy improved from 0.85770 to 0.91349, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 322ms/step - accuracy: 0.4996 - loss: 1.7478 - val_accuracy: 0.9135 - val_loss: 0.4767 - learning_rate: 2.0000e-05\nEpoch 23/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.5082 - loss: 1.7162\nEpoch 23: val_accuracy did not improve from 0.91349\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 312ms/step - accuracy: 0.5082 - loss: 1.7162 - val_accuracy: 0.8779 - val_loss: 0.7444 - learning_rate: 2.0000e-05\nEpoch 24/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.5061 - loss: 1.7199\nEpoch 24: val_accuracy improved from 0.91349 to 0.91571, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 313ms/step - accuracy: 0.5061 - loss: 1.7199 - val_accuracy: 0.9157 - val_loss: 0.4998 - learning_rate: 2.0000e-05\nEpoch 25/30\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.5055 - loss: 1.7267\nEpoch 25: val_accuracy improved from 0.91571 to 0.92437, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 320ms/step - accuracy: 0.5055 - loss: 1.7267 - val_accuracy: 0.9244 - val_loss: 0.4644 - learning_rate: 2.0000e-05\nEpoch 26/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.5143 - loss: 1.6990\nEpoch 26: val_accuracy did not improve from 0.92437\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 319ms/step - accuracy: 0.5143 - loss: 1.6990 - val_accuracy: 0.9156 - val_loss: 0.4597 - learning_rate: 2.0000e-05\nEpoch 27/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.5142 - loss: 1.6915\nEpoch 27: val_accuracy did not improve from 0.92437\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 316ms/step - accuracy: 0.5142 - loss: 1.6915 - val_accuracy: 0.8847 - val_loss: 0.7594 - learning_rate: 2.0000e-05\nEpoch 28/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.5157 - loss: 1.6857\nEpoch 28: val_accuracy did not improve from 0.92437\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 313ms/step - accuracy: 0.5157 - loss: 1.6857 - val_accuracy: 0.9070 - val_loss: 0.5841 - learning_rate: 2.0000e-05\nEpoch 29/30\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.5188 - loss: 1.6789\nEpoch 29: val_accuracy did not improve from 0.92437\n\nEpoch 29: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 317ms/step - accuracy: 0.5188 - loss: 1.6789 - val_accuracy: 0.9134 - val_loss: 0.5072 - learning_rate: 2.0000e-05\nEpoch 30/30\n\u001b[1m951/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.5178 - loss: 1.6845\nEpoch 30: val_accuracy improved from 0.92437 to 0.92536, saving model to /kaggle/working/models/best_model_final.keras\n\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 315ms/step - accuracy: 0.5178 - loss: 1.6845 - val_accuracy: 0.9254 - val_loss: 0.4194 - learning_rate: 4.0000e-06\nRestoring model weights from the end of the best epoch: 30.\n✅ Phase 2 completed successfully\nPhase 1 history keys: ['accuracy', 'loss', 'val_accuracy', 'val_loss']\nPhase 2 history keys: ['accuracy', 'loss', 'val_accuracy', 'val_loss', 'learning_rate']\nCommon keys: {'accuracy', 'loss', 'val_accuracy', 'val_loss'}\n✅ Training completed!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Evaluate and Export","metadata":{}}]}